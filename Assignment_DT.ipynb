{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain the Decision Tree algorithm in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decision Tree >>\n",
    "# First Case >> Independent variable >> categorical values\n",
    "#            >> Dependent variable   >> categorical values\n",
    "* First Step we have to select root node >> we have 2 options \n",
    "  > Information gain >> entropy \n",
    "  > Gini index / impurity\n",
    "\n",
    "> Information gain\n",
    "* Feature having maximum information gain amongst all features will be selected as root node.\n",
    "> formula for information gain\n",
    "> Information gain of a feature \n",
    ">> Entropy of feature - sumation[weighted average of each category * entropy of each category]\n",
    "\n",
    "* entropy >> It is a measure of impurity/uncertainity/randomness\n",
    "> formula >> entropy of each category = -prob(yes)*log2prob(yes) - prob(no)*log2prob(no)\n",
    "* After calculating Information gain of each feature from the dataset we will select the maximum information gain\n",
    "* as our root node.\n",
    "\n",
    "> Gini Index >> It is a measure of impurity\n",
    "> formula for gini index of a feature>>\n",
    "> gini index of a category = 1 - prob(yes)2 - prob(no)2\n",
    "> gini index of a feature = sumation[weighted average of each category * gini index of each category]\n",
    "\n",
    "# Second Case >> Independent variable >> continous values\n",
    "#             >> Dependent variable   >> categorical values\n",
    "\n",
    "* we will divide the feature having continous values in different thresholds(sort the values in independent  variable * in ascending order)\n",
    "* example >> 1,2,3,4,5,6,7,8,9,10  threshold for 1,2 will be 1.5 ,for 2,3 will be 2.5 and likewise for rest\n",
    "* for above example we will have 9 thresholds\n",
    "* we will calculate Information gain for each threshold \n",
    "* Maximum Information gain among all thresholds will be selected the information gain for that feature.\n",
    "* Likewise all information gain values of all features will be calculated.\n",
    "* Maximum Information gain among all features will be selected as root node.\n",
    "* In case of Gini Index gini index of each thresholds will be calculated.\n",
    "* Minimum Gini index among all thresholds will be selected as gini index of that feature.\n",
    "* Likewise all gini index of all features will be calculated.\n",
    "* Minimum Gini index among all features will be selected as root node.\n",
    " \n",
    "\n",
    "# Third Case >> Independent variable >> continous values\n",
    "#             >> Dependent variable >> continuous values\n",
    "* As we have continous values in target column we cannot use entropy,gini index to find out the root node.\n",
    "* Suppose we have plotted our values on graph where we have age on x-axis and cibil score on y-axis.\n",
    "* we will use a splitter to divide the data into 2 parts .\n",
    "* values on the left side of the splitter to be predicted will be the mean of the values on the left side of splitter.\n",
    "* same goes for the right side of the splitter\n",
    "* How to find the Best Splitter ?\n",
    "* Sort the Independent variables in ascending order.\n",
    "* Calculate the threshold for each 2 data points ,Suppose we have 1,2,3,4,5,6,7,8,9,10 we will calculate threshold \n",
    "* for 1,2 then 2,3 then for 3,4 likewise we will get 9 thresholds\n",
    "* For every threshold we will calculate MSE/MAE \n",
    "* Every threshold will be consirded as splitter.\n",
    "* Data points on either side of the splitter will be predicted by mean of the data points on that side of the splitter.\n",
    "* So our mean will be the predicted value and we will also have actual values .\n",
    "* So we will calculate MSE for Each threshold or splitter for that feature.\n",
    "* Minimum MSE amongst all thresholds will be selected as the splitter or best threshold for that feature.\n",
    "* In this way we will calculate Best MSE for each feature and minimum MSE amongst all features wiil be selected as the root node .\n",
    "\n",
    "\n",
    "# Fourth Case >> Independent variable >> categorical values\n",
    "#            >> Dependent variable  >> continuous values\n",
    "* As we have continous values in target column we cannot use entropy,gini index to find out the root node.\n",
    "* Suppose we have have 2 categories as independent variables male and female\n",
    "* we will split the data by splitter in 2 parts \n",
    "* Take the mean of all data points on either side of the splitter \n",
    "* We will calculate MSE as we have actual values and predicted value will be the mean of all data points on either side of the splitter.\n",
    "* Now we have MSE value of that feature.\n",
    "* If this value of MSE is lowest amongst all features it will be selected as root node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the Steps for Making a decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
