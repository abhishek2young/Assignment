{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is Ensemble Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensemble means a group of somthing.\n",
    "* In Machine Learning it is colletion of algorithms.\n",
    "* The idea for this is taken from the widom of crowds.We can see some examples for this \n",
    "1. KBC Audience poll\n",
    "2. Widespread phenomenon \n",
    "3. Product reviews one commerce \n",
    "4. IMDB ratings\n",
    "5. Voting in Democracy \n",
    "\n",
    "* Say we have different models we will take mean of their prediction in continous dataset and majority win voting count in classification here all model will be always unique.\n",
    "* Example : Say KBC have audience includes only Software Engineer so they will not be able to answer questions from different backgrounds.But we have different people from different fields then we will get good prediction from audience.\n",
    "\n",
    "> Why use them ?\n",
    "* Ensemble techniques can significantly improve the performance of machine learning models reducing overfitting , improving accuracy and increasing the robustness of the system.\n",
    "> What are they ?\n",
    "* We can use different models or say algorithm same data same.\n",
    "1. Algorithm same \n",
    "2. Data are same \n",
    "3. Both Different \n",
    "> Types of Ensemble techniques ?\n",
    "1. Voting Ensemble \n",
    "* Takes majority in classification and mean in continuous variables.\n",
    "\n",
    "2. Bagging (Random Forest)\n",
    "* Bagging stands for bootstrap aggregation.Here we will take same model but different data base.Bootstraping will be done which will give us new database.\n",
    "* As we have many decision tree so we make a forest of collection of trees.\n",
    "\n",
    "3. Boosting (Ada boost >> Adaptive boosting , Gradient boosting ,XG boost(Xtreme boosting))\n",
    "* Most powerful techniques \n",
    "* Here we will put 3 model and give data to the first model and calculate the first model mistakes and now will tell this mistakes to second model and again second model mistakes to third model and this is how in series our model accuracy will be increased.\n",
    "\n",
    "4. Stacking \n",
    "* Say we have calculated using with 3 algorithm and taken majority of their output as dataset for new more Machine Learning model.\n",
    "* Example > Discriminantion like Ambani votes values more.That new model will be trained on the data created by all our old models and will give weighted average on three accuracies and analyze which is better.\n",
    "* Stacking is one of the popular ensemble modelling techniques in machine learning.\n",
    "\n",
    "* Various weak learners are ensembelled in a parallel manner in such a way that by combining them with meta learners we can predict better predictions of the future.\n",
    "* The train set is split into 10 parts. Training for 9 parts testing data on 10th part.Different algorithms used for testing accuracy \n",
    "\n",
    "> Disadvantages \n",
    "* Multiple model trains which causes computational complexity.\n",
    "\n",
    "> Advantages\n",
    "* Biased or variance got reduced to low\n",
    "* Improvement in performance\n",
    "* Robustness\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is difference between Bagging and Boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bagging have same models but different dataset in Boosting we have same dataset but different models.\n",
    "2. In bagging aim is to try reduce variance and in Boosting we try to reduce bias.\n",
    "3. In bagging base classifiers are trained parallely and in Boosting base classifiers are trained sequentially.\n",
    "4. Bagging takes prediction of all models and take their mean in continous dataset and majority in terms of classifications, here all models will be unique.In Boosting all base learners(weak learners) are arranged in sequential format so that they can learn from the mistakes of their precedding base learners.\n",
    "5. Each model in Bagging receives equal weight while in Boosting models are weighted according to their performance.\n",
    "6. Each model is bulid independently in Bagging and in Boosting new models are influenced by the performance of the previous bulit model.\n",
    "7. Different training data subsets are selected using row sampling with replacement and random sampling methods from the entire training dataset in Bagging, while in Boosting every new subset contains the elements that were misclassified by previous models.\n",
    "8. Example for Bagging (Random Forest) and for Boosting (Ada Boosting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bagging have different datasets but same models , while in Boosting we have different models and same datasets.\n",
    "2. In Bagging we aim to decrease the variance , while in Boosting we aim to decrease the bias.\n",
    "3. In Bagging base classifiers are trained parallel , while in Boosting base classifiers are trained sequentially.\n",
    "4. bagging takes predictions of all models and take their mean if continuous dataset and will use majority in case of classification , while in Boosting all base learners(Weak) are arranged in a sequential order so that they can learn from previous mistakes of their precedding learner.\n",
    "5. In Bagging each model recieves equal weight , while in Boosting models are weighted according to their performance.\n",
    "6. In Bagging each model is build independently, while in Boosting new models are influenced by the performance of previous models.\n",
    "7. Different training data subsets are selected using row sampling with replacement and random sampling methods from the entire training dataset, while in Boosting every new subset contains the elements that were misclassified by previous models.\n",
    "8. Example for Bagging is Random Forest and for Boosting is AdaBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain the working of the AdaBoost Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Working principle of AdaBoost \n",
    "* Consider a dataset having different data points and initlaize it.\n",
    "* It starts with bulding a primary model from 100% avaialble trainig data sets.\n",
    "* Now give equal weights to each data point.\n",
    "* Assume this weight as an input for the model.\n",
    "* Identifies the correctly and wrongly classified data points in the base model.\n",
    "* Secondary model (Dataset 2) is built with more weightage on wrongly classified data points, now all the points which have higher weightage are given more importance in this secondary model and further again it identifies the correctly and wrongly classified data points present in the secondary model.\n",
    "* After identifying the wrongly classified data points a third model is introduced in this process.\n",
    "* In this way this process of introducing more models is continued until we get a complete training datset by which model predicts correctly.\n",
    "* This procedure is continued until and unless the errors are minimized and the dataset is predicted correctly.\n",
    "\n",
    "> Step2 \n",
    "* Find out the weight for each point is known as Sample weight.\n",
    "* Sample weight = Number of point / Total no of sample \n",
    "* Sample weight = 1 / total no of sample \n",
    "\n",
    "> Step3 \n",
    "* Train the model using Decision Tree algorithm (Decision Stump)\n",
    "* Find out Correctly and Wrongly classified data points \n",
    "\n",
    "> Step4 \n",
    "* Create new datset where we give more weightage to wrongly classified data points \n",
    "* For that purpose there are 5 Basic steps \n",
    "\n",
    "a. Find out total error(TE) \n",
    "> TE = Number of wrongly classified samples / Total number of samples \n",
    "\n",
    "b. Find out performance of the model \n",
    "> Performance = 1 / 2 * log (1 - TE / TE)\n",
    "\n",
    "c. For new samples find out new weight for correctly and wrongly classified data points \n",
    "> weight for correctly classified datapoints = Old sample weight * e^-performance\n",
    "\n",
    "> weight for wrongly classified datapoints = old sample weight * e^performance\n",
    "\n",
    "d. Normalization \n",
    "* All the weight value should be normalized \n",
    "> Normalization = Weight value / Addition of all the weight value\n",
    "\n",
    "e. Create bucket range \n",
    "* Create Bucket to store new dataset\n",
    "* So far that purpose we required the value of Normalization for both correctly and wrongly classified data points.\n",
    "* We create Bucket range as shown in below steps \n",
    "\n",
    "f. Create new data set points \n",
    "* Take any values between 0 to 1 \n",
    "* we have to have 8 points in dataset to take 8 values \n",
    "* Then fit the correctly and wrongly classified data points within the range of Bucket.\n",
    "\n",
    "1. We will take entire training dataset and create decision stump with depth = 1\n",
    "2. We will assign sample which have weight = 1 / Total number of samples\n",
    "3. Calculate total error by performance calculation \n",
    "4. We need to calculate performance\n",
    "5. We will calculate weight for wrongly classified data points and correctly classified data points.\n",
    "6. We will do normalization\n",
    "7. We will create Bucket range \n",
    "8. This random values will help to take the most of the values for new dataset wrongly classified will take that is known as up sampling. This process will used reclusively again up to n estimator or model and will stop if no missclassification.\n",
    "9. We will add all alpha values now take alpha values of all three output \n",
    "\n",
    "> Key takeaways\n",
    "1. AdaBoost is a Boosting algorithm known as the stagewise additive method,as it follows weak learners addition in subsequent training stages.\n",
    "2. Most of the time decision stumps are used as weak learners in AdaBoost for getting an idea about the backed of the algorithm and higher accuracies.\n",
    "3. In AdaBoost the error rate is less than the alpha term hence less weightage to the weak leaner.So in the weight updation weak learners who make mistakes will be given higher weightage through the alpha term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Weak learners in Machine Learning are Machine Learning algorithms with very low accuracy almost just above 50% almost all the time. It means that it performs poorly in the test dataset and gives unreliable results.\n",
    "* Example >> Like head or tail we will have 50%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the difference between a Weak Learner vs a Strong Learner and why they could be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. An algorithm that performs slightly better than random guessing is called as Weak learner. An algorithm that can learn complex patterns in the data and makes high accurate predictions.\n",
    "2. Error rate is slighlty below 50% in weak learners. Error rate is low in strong learners.\n",
    "3. Low complexity tends to underfit the data in Weak learners. High complexity can overfit the data if not regulaized properly.\n",
    "4. Multiple weak learners can be combined (Using Boosting) to form a strong learner.\n",
    "5. Example of Weak learners are Decision stump and Logistic regression with few features. Example of Strong learners are Gradient boosting , Deep neural network ,SVM,Bagging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the Stumps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decision stumps are a type of Decision Tree having a maximum depth of 1 .Here we can clearly understand that decision trees having max depth 1 will be weak learners .Mostly Decision trees are used as Weak learners.\n",
    "* Hence Decision Stumps are weak learners.They will have only one split geometrically in the dataset and the split wil be based on accuracy.\n",
    "* However combining multiple weak learners gives higher and more reliable accuracy so in adaboost multiple weak learners are trained.In the end we will get a strong learner having high accuracy.In Adaboost any algorithm can be selected as weak learners. However in adaboost decision stumps are often chosen as weak learners.\n",
    "* Choosing Decision trees as a weak learner is for getting higher accuracy and to see what the model is doing in the backend while training and when trained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to calculate Total Error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total error is term in AdaBoost which is used to calculate the performance of the model and by using it we can calculate new weightage of classified and misclassified data points those helps to create new dataset for our next estimator.\n",
    "* Example > Say we have 10 sample and misclassified 5 then error will be 50%\n",
    "a. Find out the total error (TE) :\n",
    "> TE = Number of wrongly classified data points / Total number of data points\n",
    "\n",
    "b. Find out the performance of the model\n",
    "> Performance = 1/2 * log(1 - TE / TE)\n",
    "\n",
    "c. For new samples find out new weight for correctly classified data points and misclassified data points\n",
    "> New weight for correctly classified data points = Old weight * e^-performance\n",
    "> New weight for misclassified data points = Old weight * e^performance\n",
    "\n",
    "> What is error on adaboost ?\n",
    "* Error is the sum of weight of misclassified data points.\n",
    "* In AdaBoost multiple weak learners are trained to get the strong learner.As a result hence calculating the error term of every weak learner is essential to know which weak learners is performing best and which is not.\n",
    "* The term alpha is a parameter that indicates the weight that should be given to a particular weak learner algorithm.\n",
    "If the value term alpha is greater for a particular algorithm then it indicates the model is performing best and the error rate for the same is low.\n",
    "* The formula to calculate the alpha term is related to the error term \n",
    "> alpha = 1/2 * log(1 - TE / TE)\n",
    "* We can see that alpha term is completely depndent on the error rate of particular weak learner which means that the more the error less the alpha term value and hence the less weight to the algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to calculate the Performance of the Stump?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Performance is a term in Adaboost which is calculated from total error to calculate the weightage of classified and misclassified data points.It is also useful as ending when we will take is alpha to add all output after multiplying it.\n",
    "a. Find out total error (TE) \n",
    "> TE = Number of wrongly classified data points / Total number of data points\n",
    "\n",
    "b. Find out performance of the model \n",
    "> Performance = 1/2 * log(1 - TE / TE)\n",
    "\n",
    "c. For new samples find out new weight for correctly classified data points and wrongly classified data points.\n",
    "> New weight for correctly classified data points = old weight * e^-performance\n",
    "\n",
    "> New weight for misclassified data points = old weight * e^performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does AdaBoost Works? Explain the Core Intuition of the Algorithm with Decision Stumps as Weak Learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If decision stumps are the weak learners for the adaboost algorithm then there will be decision trees with max depth of 1.\n",
    "* In the first stage the first weak learner will be taken , and they will split the dataset into two parts by splitting. Here the splitting will be done based on accuracy. Once the first weak learners have done the split ,it will have many errors as it was a decision tree with max depth of 1.\n",
    "* Now whatever errors are in the first stage will be passed on to the 2nd stage for training the second weak learner.while training the second weak learner the errors made during the first stage will be taken care of.\n",
    "* So in every stage the error term will be calculated which will reduce as we move further stages by avoiding the mistakes made by the previous weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to calculate the New Sample Weight?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Find out the total error \n",
    "> TE = Number of wrongly classified samples / Total number of samples\n",
    "\n",
    "b. Find out the performance of model\n",
    "> Performance = 1/2 * log(1 - TE / TE)\n",
    "\n",
    "c. For new samples find out new weight for correctly classified samples and wrongly classified samples\n",
    "> New weight for correctly classified samples = old weight * e^-performance\n",
    "\n",
    "> New weight for wrongly classified samples = old weight * e^performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a New Dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We will take the entire training data set and create decision stump with depth = 1\n",
    "2. We will assign sample which is 1 / total number of rows \n",
    "3. Calcluate total error for performance calculation\n",
    "4. We need to calculate performance\n",
    "5. We will calculate weights for wrongly classified and correctly classified.\n",
    "6. We will do normalization\n",
    "7. We will create bucket range \n",
    "8. This random values will help to make the most of values for new data set wrongly classified , will take that is known as up sampling. This process will be used reclusively again up to an n estimator or model and will stop if no missclassification.\n",
    "9. We will add all alphas , now take alpha values of all three output \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Does the Algorithm Decide Output for Test Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We will take the entire training data set and create a decision stump with depth = 1\n",
    "2. We will assign sample with 1 / Total number of rows\n",
    "3. Calculate total error for performance calculation \n",
    "4. We need to calculate performance\n",
    "5. We will calculate weights of correctly classified and wrongly classified data points.\n",
    "6. We will do normalization\n",
    "7. We will create bucket range \n",
    "8. This random values will help to make the most of values for new dataset wrongly classified will take that is known as up sampling .This process will used to reclusively again up to n estimator or model and will stop if no misclassification.\n",
    "9. We will add all alphas now take alpha values of all three outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whether feature scaling is required in AdaBoost Algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature scaling might improve the performance and convergence of certain optimization algorithms within the weak learners especially when features have significantly different scales.\n",
    "* For consistency and good practises its often recommended to scale features especially when using algorithms that rely on distance metrics or optimization parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List down the hyper-parameters used to fine-tune the AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hyperparameter in AdaBoost\n",
    "* Base estimator \n",
    "* sklearn.ensemble.AdaBoostClassifier\n",
    "* sklearn.ensemble.AdaboostClassifier(estimator=None, *,n_estimators=50,learning_rate=1.0,algorithm=\"SAMME.R\",\n",
    "random_state=None,base_estimator=\"deprecated\")[source]\n",
    "\n",
    "a. Estimator = None (default)\n",
    "* The base estimator from which the boosted ensemble is built.Support for sample weighting is required as well as proper classes_ and n_classes_ attributes.If none then the base estimator is DecisionTreeClassifier initialized with the maximum depth of 1.\n",
    "* Decision stumps is default value estimator is first model.\n",
    "\n",
    "b. n_estimators = 50 (default) \n",
    "* The maximum number of estimators at which boosting is terminated.In case of perfect fit the learning procedure is stopped early.Values must be in the range[1 , inf].Here less n causes underfitting and more cause Overfitting.\n",
    "* Number of model we want to take for adaboost.\n",
    "\n",
    "c. learning_rate = 1.0 (Default)\n",
    "* Weight applied to each classifier at each boosting iteration A higher learning rate increases the contribution of each classifier. There is a trade off between the learning rate and n_estimators parameters.Values must be in the range (0.0 , inf).Here we calculate the alpha in Adaboost this alpha is = 1/2 * log(1 - TE / TE) , So learning rate get multiplied by alpha and if we make 0.1 it reduces alpha values which makes alpha value shrinkage it will reduce amplitude of sample weight.\n",
    "\n",
    "d. algorithm {\"SAMME\",\"SAMME.R\"} (default = \"SAMME.R\")\n",
    "* If SAMME.R then it uses the SAMME.R real boosting algorithm.Estimator must support calculation of class probabilities.If \"SAMME\" then it uses the SAMME discrete boosting algorithm.The SAMME.R algorithm typically converges faster than SAMME achieving a lower test error with fewer boosting iterations.\n",
    "* The algorithm hyperparameter in Adaboost allows you to choose between two different algorithms for updating the weight of the weak learners during each iteration.The two available options are SAMME(Stagewise additive modelling using a multi class exponential loss function) and SAMME.R (SAMME with real valued class probabilities)\n",
    "\n",
    "> Here's a brief explanation of each \n",
    "* SAMME is the original algorithm used in adaboost for multiclass classification.It updates the weight of each weak learner and the weights are used to make a weighted majority vote.\n",
    "* SAMME.R stands for SAMME with real values.It is an extension of SAMME that incorporates class probabilities( as opposed to just the predicted classes). This modification allows for more flexibility in handling the continuous valued predictions.SAMME.R tends to be more preferred choice while dealing with muli class classification problems.\n",
    "\n",
    "e. random_state = None (default)\n",
    "* Controls the random seed given at each estimator at each boosting iteration.Thus it is only used when estimator exposes a random state.Pass an int for reproducible output across multiple function calls.\n",
    "\n",
    "f. base_estimator = None (default) \n",
    "* The base estimator from which the boosted ensemble is built.Support for sample weighting is required as well as proper classes_ and n_classes attributes.If none then the base estimator is DecisionTreeClassifier initialized with max_depth = 1.\n",
    "\n",
    "2. Attributes\n",
    "\n",
    "a. estimator = None (default)\n",
    "* The base estimator from which the ensemble is grown.\n",
    "\n",
    "b. base_estimator_estimator \n",
    "* Estimator used to grow the ensemble \n",
    "\n",
    "c. estimators_list of classifiers \n",
    "* The collection of fitted sub estimators \n",
    "\n",
    "d. classes_ndarray of shape (n_classes)\n",
    "* The Classes labels \n",
    "\n",
    "e. n_classes \n",
    "* Number of classes \n",
    "\n",
    "f. estimator_weights_ndarray of floats \n",
    "* Weights for each estimator in the boosted ensemble.\n",
    "\n",
    "g. estimator_errors_ndarray of floats \n",
    "* Classification error for each estimator in the boosted ensemble .\n",
    "\n",
    "h. feature_importances_ndarray of shape (n_features)\n",
    "* The impurity based feature importances \n",
    "\n",
    "i. n_features_in_int \n",
    "* Number of features seen during fit.\n",
    "\n",
    "j. feature_names_in_ndarray of shape (n_features_in)\n",
    "* Name of features seen during fit.Defined only when x has feature names that are all strings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the importance of the learning_rate hyperparameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The learning rate is an important hyperparameter in adaboost(Adaptive Boosting) and other iterative ensemble learning algorithms.It controls the contribution of each week learner to the final ensemble.Here's why the learning rate is crucial in AdaBoost.\n",
    "* A smaller learning rate often results in more robust model and helps prevent Overfiting.It ensures that each week learners contribution is more subtle,reducing the risk of memorizing noise in the training data.\n",
    "* The Learning Rate determines how much weight each week learner contributes to the final combined model. A Lower Learning Rate means that each weak learners contribution is diminished, while a Higher Learning Rate means that more weight is given to each weak learners. Balancing this influence is crucial for achieving a well organized model. \n",
    "* The learning rate is inversely proportional to the number of weak learners needed for convergence.A smaller learning rate typically requires more weak learners to achieve the same level of accuracy as a higher learning rate.\n",
    "However a higher learning rate may lead to faster convergence but with a increased risk of overfitting.\n",
    "* The Learning rate allows practioners to fine tune the performance of the AdaBoost model.Depending on the dataset and the characteristics of the weak learners, adjusting the learning rate can lead to better generalization and improved model performance.\n",
    "* The Learning rate is an important hyperparameter in AdaBoost(Adaptive Boosting) and other iterative ensemble learning algorithm.It controls the contribution of each weak learner to the final ensemble.Here's why the learning rate is crucial in adaboost.\n",
    "* Control Overfitting > A smaller learning rate often results in a more robust model and helps Overfitting.It ensures that each weak learner contribution is more subtle,reducing the risk of noise in the training data.\n",
    "* Influence of weak learners > The learning rate determines how much weight each weak learner contributes to the final combined model. A lower learning rate means that each weak learner contribution is diminished, while a higher learning rate gives more weight to each weak learner.Balancing this influence is crucial for achieving a well generalized model.\n",
    "* Number of weak learners > Learning rate is inversely proportional to the number of weak learners needed for convergence. A smaller learning rate typically requires more weak learners to achieve the same level of accuarcy as a higher learning rate. However a higher learning rate may lead to faster convergence but with a increased risk of Overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the advantages of the AdaBoost Algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Advantages of AdaBoost \n",
    "1. High accuracy \n",
    "* AdaBoost often achieves higher accuracy compared to individual weak learner.By combining the predictions of multiple weak learners it creates a strong ensemble model that performs well on a variety of datasets.\n",
    "\n",
    "2. Versatility \n",
    "* AdaBoost can be used with various base learners,making it a versatile algorithm.It is not limited to a specific type of weak learner , and different base learners can be employed based on the characteristics of the data.\n",
    "\n",
    "3. No need for complex weak learners\n",
    "* AdaBoost can work well even with weak learners that are only slightly better than random guessing.This is adventageous as it follows for the inclusion of simple models, reducing the computational complexity.\n",
    "* Handling of Class Imbalance \n",
    "* Adaboost gives higher weight to misclassified instances,which helps in handling class imbalance.It focuses on improving the classification of instances that are more challenging to the model.\n",
    "\n",
    "4. Few Hyperparameter to tune \n",
    "* Adaboost has relatively few hyperparameters to tune compared to some other complex algorithms.Common Hyperparameter include the number of weak learners,the learning rate ,and the type of weak learners.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the disadvantages of the AdaBoost Algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. High Computational Cost \n",
    "2. Time consumption is more \n",
    "3. Sensitive to noisy data and Outliers \n",
    "4. Suspectible to Overfitting \n",
    "5. Requires sufficient data \n",
    "6. Bias towards weak learners \n",
    "7. Not Suitable for high dimensional data \n",
    "8. Sensitive to the choice of weak learners "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the applications of the AdaBoost Algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Binary classification , Face Detection , Object recognition , Text Classification , Bio medical image analysis , Anomaly Detection , Customer Churn Prediction , Credit Scoring , Speech Recognition , Robotics , Gesture Recognition , Predictive maintenance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can you use AdaBoost for regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes AdaBoost can be used for regression tasks .It employes a series of weak learners ,typically shallow decision trees , to predict continuous numerical values. The Final prediction is a weighted sum of the predictions of these weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to evaluate AdaBoost Algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here in binary classification we have -1 and +1 zero is not there in Adaboost.\n",
    "> Geometric Intuition \n",
    "* Suppose we have 3 points as NO and 4 points as YES\n",
    "* Now we will fit decision stump and we get first decision stump and we get some values which are incorrectly classified , so now we will importance of the misclassified points, by putting weights and this values get unsmapled.\n",
    "* Now the new data with different weightage for correctly classified points and incorrectly classified points will be given to the second model the second model correctly classifies the previously incorrectly classified points but it will misclassify some data popints for the new data set.\n",
    "* Again we will calculate the weightage and provide alpha value to it and again the same procedure will be followed.\n",
    "\n",
    "> How are weights updated in Adaboost?\n",
    "* in Adaboost whatever mistakes the previous weak learners have made are passed to the next weak learners to avoid the same mistake. Now to inform the next weak learners about the previous weak learners mistake we will increase the weightage of the samples on which mistakes(Misclassification) are made by performing the Unsampling.\n",
    "* Now here the aim is to increase the weight of misclassified points to update the weight poperly.\n",
    "* For correctly classified points in which the algorithm does not make any mistake the weight updation formula will be as follows \n",
    "\n",
    "> New weight of the Correctly classified points = old weight * e^-performance\n",
    "> New weight of the misclassified points        = old weight * e^performance\n",
    "\n",
    "> Why it is called as Stage wise additive ?\n",
    "* As it adds multiple weak learners in sequence \n",
    "\n",
    "> Error on Adaboost ?\n",
    "* Error is the sum of weight of miscllasified rows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is AdaBoost known as Stagewise Additive Method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As AdaBoost is a boosting algorithm , it adds the results of multiple weak learners to get the final strong learner. Hence it is known as additive method.\n",
    "* Here the term stagewise means that in AdaBoost , one weak learner is trained , now whatever the errors that are present in the first stage while training, the first weak learner will pass to the second weak learner while training to avoid the same error in the future stages of training weak learners.Hence it is a Stagewise method.\n",
    "* Here we can see the stage wise addition of weak learners scenario in AdaBoost hence it is known as stagewise addition method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
