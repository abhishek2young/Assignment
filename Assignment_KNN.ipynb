{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the difference between Supervised And Unsupervised Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Supervised Learning \n",
    "* Think of a teacher guiding a student.The Teacher already knows the answer and helps the student learn by providing\n",
    "  examples and feedback.\n",
    "* In the same way in supervised learning,the algorithm is like the student and the labelled data is like the teachers,\n",
    "  guidance.The algorithm learns from examples with known answers.\n",
    "\n",
    ">> Unsupervised Learning \n",
    "* Imagine a Student exploring a new subject without teachers guidance.The Student tries to find patterns and \n",
    "  connections on their own.\n",
    "* In Unsupervised Learning, the algorithm explores the data without being given specific answers. It looks for \n",
    "  pattern and relationship in the data without explicit guidance.\n",
    "\n",
    "Summary \n",
    ">> Supervised Learning is like having a teacher (labelled data) guiding the learning process.\n",
    ">> Unsupervised Learning is like exploring and finding the patterns on your own without a teacher.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the Parametric and Nonparametric Machine Learning Algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Parametric Algorithm >>\n",
    "* Have a Fixed plan based on assumption.\n",
    "* Like using a specific receipe with known ingredients.\n",
    "\n",
    ">> Non Parametric Algorithm >>\n",
    "* Adapt to data without strict assumptions\n",
    "* Like being Flexible ,adjusting to whatever ingredients you have \n",
    "\n",
    ">> In a Nutshell ,parametric is like following a set receipe,while non-parametric is more like cooking without a \n",
    ">> food receipe ,being adaptable to different situations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain the K Nearest Neighbor Classification in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> KNN Stands for K nearest neighbors\n",
    "\n",
    "* Supervised machine learning algorithm used for both classification and regression\n",
    "* Distance Based Algorithm\n",
    "* Non Parametric Algorithm >> no assumptions on distribution of data.\n",
    "\n",
    "* Distance can be calculated by using Eucledian and manhattan distance\n",
    ">> formula for Eucledian Distance is >> p1p2 = sqrt[(x2-x1)^2 + (y2-y1)^2]\n",
    ">> formula for manhattan distance is >> p1p1 = |x2-x1| + |y2-y1|\n",
    "* Manhattan distance is always greater than Euclidean distance\n",
    "\n",
    "* Minkowski Distance >> (sumation of i=1 to n |xi - yi|^P)1/P\n",
    "* if P=1 then Manhattan Distance \n",
    "* If P=2 then Eucledian Distance\n",
    "\n",
    ">> Steps For KNN \n",
    "1. Load the data \n",
    "2. Select the no of kneighbors\n",
    "3. Calculate the distance \n",
    "4. Sort the data in ascending order \n",
    "5. Pick the First Kentries \n",
    "\n",
    ">> In case of classification majority will be considered\n",
    ">> In case of Regression mean will be considered of first Kentries\n",
    "\n",
    ">> Scaling is Required in case of Distance based algorithm \n",
    ">> Examples of Distance based algorithm\n",
    "1. KNN \n",
    "2. K-means clustering\n",
    "3. PCA\n",
    "4. SVM\n",
    "\n",
    ">> Scaling can be done by 2 methods \n",
    "1.Normalization \n",
    "* In sklearn we have MinMax Scalar for performing Normalization \n",
    "* After Normalization we will get values between 0 and 1\n",
    "* formula >> X(new) = (Xi - Xmin) / (Xmax - Xmin)\n",
    "\n",
    "2. Standardization \n",
    "* In Sklearn we have StandardScalar for performing Standardization \n",
    "* Formula for Z-Score and Standardization is Same \n",
    "* After Standardization we will get values between -3 to +3\n",
    "\n",
    ">> For Best Values of Kneighbors and P we use Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between KNN and K-means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> K nearest Neighbors(KNN):\n",
    "* TYPE >> Supervised Learning\n",
    "* PURPOSE >> Prediction Based on similarity of neighbors\n",
    "* ALGORITHM >> Instance based Learning\n",
    "* Parameters >> kneighbors and P value \n",
    "* Output >> Class Label(Classification) or value(regression)\n",
    "* USE CASES >> Image recognition,spam detection\n",
    "\n",
    ">> Kmeans:\n",
    "* TYPE >> Unsupervised Learning\n",
    "* PURPOSE >> CLustering data points into K groups \n",
    "* AlGORITHM >> Centroid Based Learning\n",
    "* Parameters >> K(Number of Clusters)\n",
    "* Output >> Cluster Assignments and Centroids\n",
    "* USE CASES >> Customer Segmentation,Image Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the “K” in KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In KNN algorithm \"K\" represents the number of nearest neighbors considered while making predictions for a new data \n",
    "point .Its a Key parameter that influences the algorithms behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we decide the value of \"K\" in KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Trial And error\n",
    "* Experiment with different values of K\n",
    "* See how the Model performs with various K values \n",
    "\n",
    "2. Cross Validation \n",
    "* Use Cross Validation to evaluate KNN with different values \n",
    "* Choose the K that gives the best results on Unseen data \n",
    "\n",
    "3. Consider Data Size \n",
    "* For Smaller Data Sets , use smaller value of K to avoid Overfitting\n",
    "* For Large Data Sets , use larger value of K \n",
    "\n",
    "4. Rule OF thumb \n",
    "* Start with square root of the number of total data points\n",
    "\n",
    "5. Domain Knowledge\n",
    "* Consider any Specific Knowledge about the Problem\n",
    "* Adjust the k based on expected characteristics of the data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is the odd value of “K” preferable in KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Using an odd value of \"K\" in KNN helps avoid ties in voting during classification ,ensuring clear majority\n",
    ">> and a decisive class assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What distance metrics can be used in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Eucledian Distance >> Measures straight line distance\n",
    "\n",
    "> Manhattan Distance >> Measures distance along the grid lines \n",
    "\n",
    "> Minkowski Distance >> Generalizes Euclidean distance and Manhattan distance\n",
    "\n",
    "> Chebyshev Distance >> Measures Maximum difference \n",
    "\n",
    "> Hamming Distance >> For Categorical Features\n",
    "\n",
    "> Cosine similarity >> Measures vector Orientation\n",
    "\n",
    "> Jaccard Distance >> For Set Data \n",
    "\n",
    "> Mahalanobis Distance >> Considers Feature Corelation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the difference between Euclidean Distance and Manhattan distance? What is the formula for Euclidean distance and Manhattan distance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eucledian distance >> Measures straight line distance\n",
    "* Manhattan distance >> Measures distance along Grid Lines \n",
    "\n",
    "> While Building Model if we pass value of P=1 then we get Manhattan distance\n",
    "> For P=2 we get Eucledian distance \n",
    "\n",
    "> Formula for Eucledian distance >> P1P2 = sqrt[(X2 - X1)^2 + (Y2 - Y1)^2]\n",
    "> Formula for Manhattan distance >> P1P2 = |X2 - X1| + |Y2 - Y1|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do you need to scale your data for the k-NN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Distance Equality\n",
    "* Ensures all features contribute equally .\n",
    "* Prevent dominance of the features with Larger Scale\n",
    "\n",
    "2. Avoid Bias \n",
    "* Keeps Decision boundaries Uniform\n",
    "* Prevensts Features with Larger Scales having a disproportionate impact \n",
    "\n",
    "3. Faster Convergence\n",
    "* Aids faster training convergence\n",
    "* Helps Optimization algorithms find the optimal solutions Efficiently\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the Gradient Descent Based, Tree-Based, and distance-based algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Gradient Based Algorithm\n",
    "1. Linear Regression\n",
    "2. Logistic Regression\n",
    "3. Neural Network\n",
    "\n",
    ">> Distance based Algorithm\n",
    "1. KNN\n",
    "2. PCA\n",
    "3. SVM\n",
    "\n",
    ">> Tree Based Algorithm\n",
    "1. Decision Tree\n",
    "2. Random Forest\n",
    "3. Gradient Boosting Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Normalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Purpose >> Scaling features to a standard range \n",
    "* Methods >> MinMax Scaling range in (0 to 1) \n",
    "             Z-Score Normalization (mean = 0,standard deviation = 1)\n",
    "* Benefits >> Equalizes Feature Influence\n",
    "              Improves Convergence in Model Training\n",
    "* When to use >> Important for all Distance based Algorithms\n",
    "* Formula >> Xnew = (Xi - Xmin) / (Xmax - Xmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Standardization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Purpose >> Rescaling Features to have mean 0 and standard deviation 1\n",
    "* Formula >> (original value - mean) / standard deviation\n",
    "* Benefits >> Ensures equal contribution of Features \n",
    "              Aids Convergence in machine learning Algorithms\n",
    "* When to use >> Useful when features have different scales or units \n",
    "                 Applied in distance based and gradient based algorithms\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use Normalization and Standardization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use Normalization When \n",
    "* Algorithm is sensitive to feature magnitude (KNN,Neural Network)\n",
    "* Preserving Interpretability of Features is crucial \n",
    "* Scaling Between specific bounds is desired \n",
    "\n",
    "> Use Standardization when \n",
    "* Algorithm assumes mean-centered data (PCA,Linear Models)\n",
    "* DataSet contains Outliers\n",
    "* Features are assumed to be normally distributed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why KNN Algorithm is called as Lazy Learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It Memorizes the entire training data \n",
    "* No distinct training phase \n",
    "* Makes prediction by comparing to memorized intances \n",
    "* Adapts dynamically to local data during prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why should we not use the KNN algorithm for large datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Computational Cost \n",
    "* High Computational complexity due to distance calculations for every data points\n",
    "2. Memory Intensive \n",
    "* Requires storing the entire dataset,leading to high memory usage \n",
    "3. Prediction Time \n",
    "* Longer prediction times as dataset size increases\n",
    "4. Storage Issues \n",
    "* Impractical Storage Requirements for real time or frequent updates \n",
    "5. Sensitivity to Irrelevant Features \n",
    "* Large Datasets may contain more irrelevant features,impacting accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the advantages and disadvantages of the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Advantages of KNN \n",
    "1. Simple implementation\n",
    "2. No assumptions about data distribution\n",
    "3. Adaapts to local structure\n",
    "4. Versatile for Classification and Regression\n",
    "5. No Distinct training phase \n",
    "\n",
    "> Disadvantages of KNN\n",
    "1. Computational complexity for large data sets\n",
    "2. Memory Intensive \n",
    "3. Sensitive to irrelevant features \n",
    "4. Requires feature Scaling \n",
    "5. Slow Prediction time \n",
    "6. Biased Towards majority class in Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the difference between Model Parameters Vs HyperParameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model Parameters\n",
    "* Learned from data during training\n",
    "* Internal variables specific to each Trained Model\n",
    "\n",
    "> Hyperparameters\n",
    "* Set before training\n",
    "* External settings guiding the learning process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Hyperparameter Tuning in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Definition >> Adjusting external settings(hyperparameters) to optimize a machine learning models Algorithm\n",
    "* Objective >> Find the best hyperparameters values for max accuracy,precision,or other metrics\n",
    "* Examples >> Learning rates ,regularization strengths,tree depths \n",
    "* Methods >> Manual tuning,Grid Search ,random search\n",
    "* Validation data >> Uses a seperate dataset to eval models performance during tuning\n",
    "* Outcomes >> Identify the best set of hyperparameters values for optimal model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to tune hyperparameters in K Nearest Neighbors Classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select Hyperparameter >> \n",
    "* Choose parameters like \"K\"(number of neighbors) and distance matrix \n",
    "2. Define range \n",
    "* Set a range of values for chosen hyperparameters\n",
    "3. Split data \n",
    "* Divide the dataset into training and testing data \n",
    "4. Grid search Or Randomsearch \n",
    "* Use grid or random search to explore hyperparameters combinations\n",
    "5. Evaluate performance\n",
    "* Train KNN with each set of hyperparameters and evaluate on the validation set\n",
    "6. Select best hyperparameter\n",
    "* Choose the set yielding the best validation performance\n",
    "7. Validate on Test Set \n",
    "* Confirm the final models performance on seperate test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the approaches for Hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Grid Search \n",
    "* Pros >> Exhaustive search \n",
    "* Cons >> Computationally Expensive \n",
    "2. Random Search\n",
    "* Pros >> More Efficient \n",
    "* cons >> Relies on chance \n",
    "3. Manual Search\n",
    "* Pros >> Simple,expert-driven\n",
    "* Cons >> May miss optimal values \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is cross-validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition >> \n",
    "* Technique to assess model performance and reduce overfitting risk\n",
    "\n",
    "k-fold cross validation\n",
    "* Dataset divided into k subsets ,model trained and evaluated k times \n",
    "\n",
    "Purpose \n",
    "* Provides more reliable performance estimate compared to single train - test split \n",
    "\n",
    "Benefits \n",
    "* Robust performance evaluation ,reduce dependency on a specific split \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are GridSearchCV and RandomizedSearchCV, differences between them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Grid Search CV :\n",
    "* Searches through a predefined grid systematically \n",
    "* Tests all possible combinations within the grid \n",
    "* Guarantees finding the Best combination\n",
    "* Computaionally Expensive for large search spaces \n",
    "\n",
    "> Randomized Search CV :\n",
    "* Randomly samples hyperparameter values \n",
    "* More efficient for large search spaces\n",
    "* May find good combinations by chance \n",
    "* Suitable for exploration with fewer evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the Applications of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Classification \n",
    "* Assigning labels based on nearest neighbors\n",
    "2. Regression\n",
    "* Predicting values using neighbour Information \n",
    "3. Anomaly Detection\n",
    "* Identifying Outliers and unusual instances\n",
    "4. Recommendation systems \n",
    "* Recommending items based on user similarities\n",
    "5. Pattern Recognition\n",
    "* Recognizing patterns in data\n",
    "6. Clustering\n",
    "* grouping similar instances together \n",
    "7. Medical Diagnosis\n",
    "* Predicting or classifying medical conditions\n",
    "8. NLP\n",
    "* Analyzing and classifying text\n",
    "9. Environmental monitoring\n",
    "* Analyzing patterns in environmental data\n",
    "10. Robotics \n",
    "* Path planning for robots and autonomous vehicles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do you deal with outliers values in a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with Outliers \n",
    "1. Identify \n",
    "* Use visualisation and stats to find outliers\n",
    "2. Data Trimming \n",
    "* Cap extreme values at a specific percentile \n",
    "3. Transformation\n",
    "* Apply Mathematical transformation\n",
    "4. IMputation \n",
    "* Replace outliers with mean,median,mode\n",
    "5. Remove \n",
    "* Exclude Extreme values cautiously\n",
    "6. Robust Methods \n",
    "* Use Robust statistical methods less sensitive to outliers\n",
    "7. Model Based Approaches\n",
    "* Employ models less affected by outliers\n",
    "8. Seperate Models \n",
    "* Consider building models with and without outliers\n",
    "9. Domain Knowledge \n",
    "* Use Domain Expertise for infomed decisions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do you deal with missing values in a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with Missing values\n",
    "\n",
    "1.Identify \n",
    "* Use stats to find missing values\n",
    "2. Remove \n",
    "* Drop rows or columns with missing values\n",
    "3. Imputations \n",
    "* Fill with mean,median and mode \n",
    "4. Interpolation \n",
    "* Estimate missing values based on relationships\n",
    "5. Predictive Models \n",
    "* Train models to predict missing values\n",
    "6. Multiple Imputations \n",
    "* Create Multiple datasets with imputed values \n",
    "7. indicator variables \n",
    "* Add indicators for missing values\n",
    "8. Drop columns \n",
    "* Remove columns with high missingness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do you use Knn for missing value imputation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Identify Variables\n",
    "* Note the variables with missing values\n",
    "2. Preprocess data \n",
    "* Handle categorical variables and scale numeric features\n",
    "3. Seperate Data \n",
    "* Split into sets with and without missing values\n",
    "4. Train KNN model\n",
    "* Train a KNN Model on data without missing values\n",
    "5. Impute Missing Values\n",
    "* For each instance with missing values,use KNN to impute based on nearest neighbors\n",
    "6. Distance Metric \n",
    "* Choose a suitable distance metric \n",
    "7. Weighted Imputation \n",
    "* Optionally give more weight to closer neighbors\n",
    "8. Evaluate Imputed data \n",
    "* Check if imputed data alligns with the original data\n",
    "9. Handle remaining missing values\n",
    "* If any,consider other methods or adjust parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NaN Euclidean Distance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition >> Eucledian distance calculation for vectors with nan values \n",
    "\n",
    "Approaches >>\n",
    "1. Ignore Nan\n",
    "* calculate distance based on available values,ignoring NAN\n",
    "2. Handle NaN\n",
    "* Impute or replace nan before distance calculation\n",
    "3. Pairwise deletion \n",
    "* Exclude pairs with NAn from distance calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What will be the distance between the below data points:P1[3, na, na, 6] and P2[1, na, 4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance >> sqrt[(3-1)^2 + (6-5)^2]\n",
    "\n",
    "Distance >> sqrt(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
